{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13466568,"sourceType":"datasetVersion","datasetId":8548375},{"sourceId":13485230,"sourceType":"datasetVersion","datasetId":8561568},{"sourceId":13490126,"sourceType":"datasetVersion","datasetId":8565085}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y numpy scipy albumentations scikit-image\n!pip install numpy==1.26.4 scipy==1.11.4 albumentations==1.3.1 scikit-image==0.21.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install ultralytics --no-deps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:56:09.606560Z","iopub.execute_input":"2025-10-24T16:56:09.607089Z","iopub.status.idle":"2025-10-24T16:56:11.037674Z","shell.execute_reply.started":"2025-10-24T16:56:09.607065Z","shell.execute_reply":"2025-10-24T16:56:11.036705Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.221)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"Menu ‚Üí Restart Session\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install numpy==1.25.2 --upgrade\n!pip install scipy==1.11.2 --upgrade\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nprint(A.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip uninstall -y numpy\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install numpy==1.26.4\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport albumentations as A\n\nprint(\"‚úÖ OpenCV:\", cv2.__version__)\nprint(\"‚úÖ NumPy:\", np.__version__)\nprint(\"‚úÖ Albumentations:\", A.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom ultralytics import YOLO\n\nprint(\"‚úÖ Albumentations\", A.__version__)\nprint(\"‚úÖ Torch\", torch.__version__)\n\n# =========================================================\n# ‚úÖ Custom CCTV Augmentation Pipeline (without Mosaic/MixUp)\n# =========================================================\ntrain_transform = A.Compose([\n    # Geometric transformations\n    A.RandomResizedCrop(size=(640, 640), scale=(0.7, 1.0), ratio=(0.75, 1.33), p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.2),\n    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15,\n                       border_mode=cv2.BORDER_CONSTANT, p=0.8),\n\n    # Brightness / contrast / color (CCTV-like lighting)\n    A.RandomBrightnessContrast(p=0.7),\n    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=15, p=0.6),\n    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.4),\n    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.3),\n\n    # Blur / noise (simulate poor CCTV quality)\n    A.MotionBlur(p=0.3),\n    A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n    A.GaussNoise(var_limit=(5, 25), p=0.3),\n\n    # Dropouts (simulate occlusion or fog)\n    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.4),\n\n    ToTensorV2()\n])\n\n# =========================================================\n# ‚úÖ Load Pretrained YOLOv8m Model\n# =========================================================\nmodel = YOLO(\"yolov8m.pt\")\n\n# Disable YOLO‚Äôs default heavy augmentations but keep light mosaic/mixup\nmodel.overrides['augment'] = True     # Keep augment True so it can still do small mosaic/mixup\nmodel.overrides['rect'] = False\nmodel.overrides['mosaic'] = 0.3       # Small mosaic probability\nmodel.overrides['mixup'] = 0.2        # Small mixup probability\nmodel.overrides['copy_paste'] = 0.0   # Disable copy-paste\n\n# =========================================================\n# ‚úÖ Dataset Paths\n# =========================================================\ndata_yaml = \"/kaggle/input/violenceweapondetection/data.yaml\"  # replace with your dataset yaml\nsave_dir = \"/kaggle/working/yolov8m_cctv_aug_runs\"\n\n\n# =========================================================\n# ‚úÖ Hook Albumentations into YOLO dataloader\n# =========================================================\ndef custom_dataloader_hook(dataset):\n    \"\"\"Inject Albumentations transforms into YOLO‚Äôs training dataloader.\"\"\"\n    dataset.transforms = train_transform\n    return dataset\n\n# Apply Albumentations augmentations AFTER dataloader is ready\nmodel.add_callback(\"on_fit_epoch_start\", lambda trainer: custom_dataloader_hook(trainer.train_loader.dataset))\n\n# =========================================================\n# ‚úÖ Train Model\n# =========================================================\nresults = model.train(\n    data=data_yaml,\n    epochs=150,                # maximum number of epochs\n    imgsz=640,\n    batch=8,\n    device=0 if torch.cuda.is_available() else 'cpu',\n    project=save_dir,\n    name=\"yolov8m_cctv_aug\",\n    workers=2,\n    lr0=0.001,\n    optimizer=\"SGD\",\n    pretrained=True,\n    verbose=True,\n    val=True,\n    exist_ok=True,\n    patience=15              # stops training if no improvement for 25 epochs\n)\n\n# =========================================================\n# ‚úÖ Evaluate Metrics\n# =========================================================\nmetrics = model.val()\nprint(\"\\nüìä Validation Metrics:\")\ntry:\n    print(f\"Precision: {metrics.box.pr:.4f}\")\n    print(f\"Recall: {metrics.box.re:.4f}\")\n    print(f\"mAP50: {metrics.box.map50:.4f}\")\n    print(f\"mAP50-95: {metrics.box.map:.4f}\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è Metrics output error:\", e)\n    print(metrics)\n\nprint(\"\\n‚úÖ Fine-tuning completed successfully with CCTV augmentations (small mosaic & mixup)!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\n    \"/kaggle/working/yolov8m_final_cctv_aug_runs\",   # output zip path (without .zip extension)\n    'zip',                                     # format\n    \"/kaggle/working/yolov8m_cctv_aug_runs\"  # folder to zip\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This For resume the training \n","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# ‚úÖ Imports\n# =========================================================\nimport os\nimport torch\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom ultralytics import YOLO\n\nprint(\"‚úÖ Albumentations:\", A.__version__)\nprint(\"‚úÖ Torch:\", torch.__version__)\n\n# =========================================================\n# ‚úÖ CCTV Albumentations Augmentation Pipeline (Fixed)\n# =========================================================\ntrain_transform = A.Compose([\n    # Geometry (safe for bounding boxes)\n    A.RandomSizedBBoxSafeCrop(height=640, width=640, p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.2),\n    A.Affine(scale=(0.9, 1.1), translate_percent=(0.05, 0.05), rotate=(-15, 15), p=0.8),\n\n    # Lighting & Color (simulate CCTV variations)\n    A.RandomBrightnessContrast(p=0.7),\n    A.HueSaturationValue(p=0.6),\n    A.CLAHE(p=0.4),\n    A.RGBShift(p=0.3),\n\n    # Blur / Noise (simulate low-quality surveillance)\n    A.MotionBlur(p=0.3),\n    A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n    A.GaussNoise(p=0.3),\n\n    # Partial occlusion / smoke / dust effects\n    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.4),\n\n    ToTensorV2()\n], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n\n# =========================================================\n# ‚úÖ Load YOLO Model from Last Checkpoint (Resume Point)\n# =========================================================\nlast_checkpoint = \"/kaggle/input/v9leave/New folder/last (15).pt\"\nmodel = YOLO(last_checkpoint)\nprint(f\"‚úÖ Loaded checkpoint: {last_checkpoint}\")\n\n# =========================================================\n# ‚úÖ Dataset YAML (Same as before)\n# =========================================================\ndata_yaml = \"/kaggle/input/violenceweapondetection/data.yaml\"\n\n# =========================================================\n# ‚úÖ Attach Albumentations to YOLO‚Äôs Training Loader\n# =========================================================\ndef apply_albumentations(trainer):\n    trainer.train_loader.dataset.transform = train_transform\n\nmodel.add_callback(\"on_train_start\", apply_albumentations)\n\n# =========================================================\n# ‚úÖ Resume Training from Last Epoch\n# =========================================================\nresults = model.train(\n    data=data_yaml,\n    imgsz=640,\n    epochs=150,\n    batch=8,\n    device=0 if torch.cuda.is_available() else 'cpu',\n    project=\"/kaggle/working/yolo_cctv_aug_runs\",\n    name=\"yolov9m_cctv_aug_resume\",\n    resume=True,                     # ‚úÖ Continue training from where you stopped\n    workers=2,\n    val=True,\n    exist_ok=True,\n    patience=15\n)\n\n# =========================================================\n# ‚úÖ Validate After Training\n# =========================================================\nmetrics = model.val()\n\nprint(\"\\nüìä Final Validation Metrics:\")\ntry:\n    print(f\"Precision: {metrics.box.pr:.4f}\")\n    print(f\"Recall: {metrics.box.re:.4f}\")\n    print(f\"mAP50: {metrics.box.map50:.4f}\")\n    print(f\"mAP50-95: {metrics.box.map:.4f}\")\nexcept:\n    print(metrics)\n\nprint(\"\\n‚úÖ Training Resumed Successfully with CCTV Augmentations Active!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}