{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13478064,"sourceType":"datasetVersion","datasetId":8556742}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --upgrade --force-reinstall scipy==1.10.1\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ultralytics --no-deps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T07:58:53.568635Z","iopub.execute_input":"2025-10-27T07:58:53.569121Z","iopub.status.idle":"2025-10-27T07:58:56.024134Z","shell.execute_reply.started":"2025-10-27T07:58:53.569100Z","shell.execute_reply":"2025-10-27T07:58:56.023357Z"},"editable":false},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.221-py3-none-any.whl.metadata (37 kB)\nDownloading ultralytics-8.3.221-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ultralytics\nSuccessfully installed ultralytics-8.3.221\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport random\nimport numpy as np\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom ultralytics import YOLO\n\nprint(\"‚úÖ Albumentations\", A.__version__)\nprint(\"‚úÖ Torch\", torch.__version__)\n\n# =========================================================\n# ‚úÖ GLOBAL SEED FOR FULL REPRODUCIBILITY\n# =========================================================\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n# =========================================================\n# ‚úÖ CCTV Augmentation Pipeline (with bounding-box safety)\n# =========================================================\ntrain_transform = A.Compose([\n    A.RandomResizedCrop(size=(640, 640), scale=(0.7, 1.0), ratio=(0.75, 1.33), p=1.0),\n\n    # Geometric\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.2),\n    A.ShiftScaleRotate(\n        shift_limit=0.05,\n        scale_limit=0.1,\n        rotate_limit=15,\n        border_mode=cv2.BORDER_CONSTANT,\n        p=0.8\n    ),\n\n    # Lighting & color (CCTV realism)\n    A.RandomBrightnessContrast(p=0.7),\n    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=15, p=0.6),\n    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.4),\n    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.3),\n\n    # Blur and noise (CCTV quality loss)\n    A.MotionBlur(p=0.3),\n    A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n    A.GaussNoise(var_limit=(5, 25), p=0.3),\n\n    # Occlusion / fog simulation\n    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.4),\n\n    ToTensorV2()\n],\n    bbox_params=A.BboxParams(\n        format='yolo',                 # YOLO (x_center, y_center, w, h)\n        label_fields=['class_labels'],  # Keep labels with boxes\n        min_visibility=0.15            # Removes tiny leftover objects after augmentation\n    )\n)\n\n\n# =========================================================\n# ‚úÖ Load YOLOv9m Model\n# =========================================================\nmodel = YOLO(\"yolov9m.pt\")\n\n# Light built-in augmentations (as you set before)\nmodel.overrides['augment'] = True\nmodel.overrides['rect'] = False\nmodel.overrides['mosaic'] = 0.3\nmodel.overrides['mixup'] = 0.2\nmodel.overrides['copy_paste'] = 0.0\n\n\n# =========================================================\n# ‚úÖ Dataset + Save Directory\n# =========================================================\ndata_yaml = \"/kaggle/input/vioelnceweapondetectiond/data.yaml\"\nsave_dir = \"/kaggle/working/yolov9m_cctv_aug_runs\"\n\n\n# =========================================================\n# ‚úÖ Hook Albumentations into YOLO Dataloader\n# =========================================================\ndef custom_dataloader_hook(dataset):\n    dataset.transforms = train_transform\n    return dataset\n\nmodel.add_callback(\"on_fit_epoch_start\",\n                   lambda trainer: custom_dataloader_hook(trainer.train_loader.dataset))\n\n\n# =========================================================\n# ‚úÖ Train Model\n# =========================================================\nresults = model.train(\n    data=data_yaml,\n    epochs=200,\n    imgsz=640,\n    batch=16,\n    device=0 if torch.cuda.is_available() else 'cpu',\n    project=save_dir,\n    name=\"yolov9m_cctv_aug\",\n    workers=4,\n    lr0=0.001,\n    optimizer=\"SGD\",\n    pretrained=True,\n    verbose=True,\n    val=True,\n    exist_ok=True,\n    patience=15\n)\n\n\n# =========================================================\n# ‚úÖ Evaluate Model\n# =========================================================\nmetrics = model.val()\nprint(\"\\nüìä Validation Metrics:\")\ntry:\n    print(f\"Precision:   {metrics.box.pr:.4f}\")\n    print(f\"Recall:      {metrics.box.re:.4f}\")\n    print(f\"mAP50:       {metrics.box.map50:.4f}\")\n    print(f\"mAP50-95:    {metrics.box.map:.4f}\")\nexcept:\n    print(metrics)\n\nprint(\"\\n‚úÖ Training completed successfully with YOLOv9m + CCTV augmentations + bbox safety + reproducibility!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:02:31.247041Z","iopub.execute_input":"2025-10-27T08:02:31.247567Z","execution_failed":"2025-10-27T08:50:18.475Z"},"editable":false},"outputs":[{"name":"stdout","text":"‚úÖ Albumentations 2.0.8\n‚úÖ Torch 2.6.0+cu124\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n/tmp/ipykernel_87/3648390766.py:52: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n  A.GaussNoise(var_limit=(5, 25), p=0.3),\n/tmp/ipykernel_87/3648390766.py:55: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n  A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.4),\n","output_type":"stream"},{"name":"stdout","text":"Ultralytics 8.3.221 üöÄ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/input/vioelnceweapondetectiond/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=200, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.2, mode=train, model=yolov9m.pt, momentum=0.937, mosaic=0.3, multi_scale=False, name=yolov9m_cctv_aug, nbs=64, nms=False, opset=None, optimize=False, optimizer=SGD, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/kaggle/working/yolov9m_cctv_aug_runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/yolov9m_cctv_aug_runs/yolov9m_cctv_aug, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\nOverriding model.yaml nc=80 with nc=2\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1    171648  ultralytics.nn.modules.block.RepNCSPELAN4    [64, 128, 128, 64, 1]         \n  3                  -1  1    276960  ultralytics.nn.modules.block.AConv           [128, 240]                    \n  4                  -1  1    629520  ultralytics.nn.modules.block.RepNCSPELAN4    [240, 240, 240, 120, 1]       \n  5                  -1  1    778320  ultralytics.nn.modules.block.AConv           [240, 360]                    \n  6                  -1  1   1414080  ultralytics.nn.modules.block.RepNCSPELAN4    [360, 360, 360, 180, 1]       \n  7                  -1  1   1556160  ultralytics.nn.modules.block.AConv           [360, 480]                    \n  8                  -1  1   2511840  ultralytics.nn.modules.block.RepNCSPELAN4    [480, 480, 480, 240, 1]       \n  9                  -1  1    577440  ultralytics.nn.modules.block.SPPELAN         [480, 480, 240]               \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1   1586880  ultralytics.nn.modules.block.RepNCSPELAN4    [840, 360, 360, 180, 1]       \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    715920  ultralytics.nn.modules.block.RepNCSPELAN4    [600, 240, 240, 120, 1]       \n 16                  -1  1    397808  ultralytics.nn.modules.block.AConv           [240, 184]                    \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1   1480320  ultralytics.nn.modules.block.RepNCSPELAN4    [544, 360, 360, 180, 1]       \n 19                  -1  1    778080  ultralytics.nn.modules.block.AConv           [360, 240]                    \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   2627040  ultralytics.nn.modules.block.RepNCSPELAN4    [720, 480, 480, 240, 1]       \n 22        [15, 18, 21]  1   4638262  ultralytics.nn.modules.head.Detect           [2, [240, 360, 480]]          \nYOLOv9m summary: 348 layers, 20,159,766 parameters, 20,159,750 gradients\n\nTransferred 901/907 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 96.1¬±43.5 MB/s, size: 56.4 KB)\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/vioelnceweapondetectiond/train/labels... 7720 images, 1837 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7720/7720 819.0it/s 9.4s0.0s\nWARNING ‚ö†Ô∏è \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/vioelnceweapondetectiond/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.5¬±0.3 ms, read: 48.8¬±40.7 MB/s, size: 43.6 KB)\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/vioelnceweapondetectiond/valid/labels... 948 images, 205 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 948/948 805.4it/s 1.2s0.1s\nWARNING ‚ö†Ô∏è \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/vioelnceweapondetectiond/valid is not writeable, cache not saved.\nPlotting labels to /kaggle/working/yolov9m_cctv_aug_runs/yolov9m_cctv_aug/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.001, momentum=0.937) with parameter groups 149 weight(decay=0.0), 156 weight(decay=0.0005), 155 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1m/kaggle/working/yolov9m_cctv_aug_runs/yolov9m_cctv_aug\u001b[0m\nStarting training for 200 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      1/200      4.37G      1.809      3.018      1.943          7        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 965/965 2.7it/s 5:58<0.4s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 60/60 3.6it/s 16.7s0.2s\n                   all        948        806      0.476      0.478      0.441      0.226\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      2/200      5.09G       1.65      2.001       1.75          7        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 965/965 2.8it/s 5:50<0.4s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 60/60 4.0it/s 15.1s0.2s\n                   all        948        806      0.627      0.539      0.584      0.309\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      3/200      5.15G      1.572       1.74       1.69         13        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 965/965 2.8it/s 5:46<0.4s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 60/60 4.0it/s 15.2s0.2s\n                   all        948        806      0.688      0.589      0.639      0.347\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      4/200       5.2G      1.553      1.662      1.663          7        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 965/965 2.8it/s 5:46<0.4s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 60/60 4.0it/s 15.1s0.2s\n                   all        948        806      0.704      0.619      0.669      0.361\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      5/200      5.26G      1.535      1.564       1.63          8        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 965/965 2.8it/s 5:46<0.4s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 60/60 4.0it/s 15.1s0.2s\n                   all        948        806      0.685      0.674      0.704      0.391\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      6/200      5.32G      1.518      1.477      1.617          6        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 965/965 2.8it/s 5:46<0.4s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 60/60 4.0it/s 15.0s0.2s\n                   all        948        806      0.744      0.648       0.72      0.399\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      7/200      5.38G      1.492      1.403      1.589         10        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 965/965 2.8it/s 5:46<0.4s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 60/60 4.0it/s 14.9s0.2s\n                   all        948        806      0.763       0.68      0.733      0.404\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      8/200      5.43G      1.467       1.32      1.553          9        640: 28% ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 274/965 2.8it/s 1:39<4:106","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}